{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "import xlwings as xw\n",
    "\n",
    "wb = xw.Book('weibo_clean.xls')\n",
    "shts = wb.sheets\n",
    "\n",
    "# we divide the posts into 5 periods: 12/20 - 1/22, 1/23 - 2/18, 2/19 - 3/11, 3/12 - 4/7, 4/8 - 4/30\n",
    "p1, p2, p3, p4, p5 = [], [], [], [], []\n",
    "all_P = []\n",
    "for sht in shts:\n",
    "    date = sht.name\n",
    "    posts = sht.range('A1').expand('down').value\n",
    "    all_P.append(posts)\n",
    "for p in all_P[:34]:\n",
    "    p1 += p\n",
    "for p in all_P[34:61]:\n",
    "    p2 += p\n",
    "for p in all_P[61:83]:\n",
    "    p3 += p\n",
    "for p in all_P[83:110]:\n",
    "    p4 += p\n",
    "for p in all_P[110:132]:\n",
    "    p5 += p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.925*\" \" + 0.012*\"冠状病毒\" + 0.009*\":\" + 0.008*\"注意安全\" + 0.003*\"hands\" + 0.003*\"folded\" + 0.002*\"转发\" + 0.001*\"face\" + 0.001*\"泰国\" + 0.000*\"with\"')\n",
      "(1, '0.616*\" \" + 0.033*\"怕\" + 0.033*\"冠状病毒\" + 0.027*\"注意\" + 0.007*\"口罩\" + 0.006*\"转发\" + 0.006*\"…\" + 0.004*\"说\" + 0.003*\"吃\" + 0.003*\"戴\"')\n",
      "(2, '0.837*\" \" + 0.032*\"注意\" + 0.016*\"冠状病毒\" + 0.002*\"转发\" + 0.002*\"武汉\" + 0.001*\"·\" + 0.001*\"口罩\" + 0.001*\"日本\" + 0.001*\"避免\" + 0.001*\"病毒\"')\n",
      "(3, '0.989*\" \" + 0.002*\"冠状病毒\" + 0.000*\"转发\" + 0.000*\"慌\" + 0.000*\"说\" + 0.000*\"病毒\" + 0.000*\"…\" + 0.000*\"没有\" + 0.000*\"走\" + 0.000*\"-\"')\n",
      "(4, '0.453*\" \" + 0.048*\"冠状病毒\" + 0.021*\"新型\" + 0.017*\"武汉\" + 0.013*\"肺炎\" + 0.009*\"例\" + 0.008*\"辛苦\" + 0.007*\"病例\" + 0.007*\"感染\" + 0.007*\"日\"')\n",
      "<bound method BaseTopicModel.print_topic of <gensim.models.ldamodel.LdaModel object at 0x1a34e7e7b8>>\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "stop_words = [' ', ]\n",
    "with open('cn_stopwords.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        stop_words.append(line.strip())\n",
    "    \n",
    "words_ls = []\n",
    "for p in p1:\n",
    "    if p:\n",
    "        words = [w for w in jieba.cut(p) if w not in stop_words]\n",
    "        words_ls.append(words)\n",
    "\n",
    "dictionary = corpora.Dictionary(words_ls)\n",
    "\n",
    "corpus = [dictionary.doc2bow(words) for words in words_ls]\n",
    "\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5)\n",
    "\n",
    "for topic in lda.print_topics(num_words=10):\n",
    "    print(topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
